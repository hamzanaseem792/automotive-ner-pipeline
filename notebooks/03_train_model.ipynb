{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7eb2f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\hamza\\miniconda3\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 63/63 [02:04<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.2647 | Val Loss: 1.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:50<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 0.0045 | Val Loss: 1.7434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [03:35<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.0016 | Val Loss: 1.8014\n",
      " Model and tokenizer saved to '../outputs/ner_model'\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, DataCollatorForTokenClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "## look it was good step that i have saveed tokenized dataset , here i dont have to retokenized thema again\n",
    "tokenized_datasets = load_from_disk(\"../data/tokenized_dataset\")\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# lets import pretrained model and tokeniezr \n",
    "model_checkpoint = \"bert-base-cased\" # please remember it is (-)between letters not ___ once i was stuck 3 hours on this error \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=11)\n",
    "\n",
    "## lets write data colletrla \n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "#  i think ,my devise does not ahve any gpu and its ok for this project ebacsue data is not as large \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "#  lets import optimizer \n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# I am going to write my own training loop instead of using training argument beacsue it provides more control, when I ahve to handle big data like in real life i ahve to write my own loop \n",
    "def train_one_epoch(model, dataloader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            total_loss += outputs.loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# this is final step \n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch(model, train_dataloader, optimizer)\n",
    "    val_loss = evaluate(model, val_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save model and tokenizer to outputs folder\n",
    "output_dir = \"../outputs/ner_model\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\" Model and tokenizer saved to '{output_dir}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
